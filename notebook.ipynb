{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a23794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert obs to tensor\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype = torch.float)\n",
    "        # Forward pass\n",
    "        obs = F.relu(self.fc1(obs))\n",
    "        obs = self.fc2(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca771873",
   "metadata": {},
   "source": [
    "**Architectural choices:**\n",
    "\n",
    "- Define the feedforwardNN class independently and inherit it from nn.Module.\n",
    "\n",
    "- have a specific _init_hyperparameters method to initialize the hyperparameters to not overload the __init__ method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb121a",
   "metadata": {},
   "source": [
    "## Steps resolve\n",
    "\n",
    "1. When defining the advantage function, they fix the value of the Value function at the beginning of each episode to avoid unstable moving average computation\n",
    "2. Normalize the advantage function to have zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.act_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.init_parameters = self._init_hyperparameters()\n",
    "\n",
    "        self.actor = Feedforward(self.obs_dim, self.act_dim) # Policy\n",
    "        self.critic = Feedforward(self.obs_dim,1)            # Value function\n",
    "\n",
    "        self.actor_opti = Adam(self.actor.parameters(), lr = self.lr)\n",
    "\n",
    "        self.cov_vect = torch.full((self.act_dim, ), fill_value = 0.5) # standard deviation vector\n",
    "        self.cov_mat = torch.diag(self.cov_vect) # [act_dim, act_dim]\n",
    "\n",
    "    def _init_hyperparameters(self):\n",
    "\n",
    "        self.timesteps_per_batch       = 4000\n",
    "        self.max_timesteps_per_episode = 1000\n",
    "        self.gamma                     = 0.95\n",
    "        self.n_updates_per_iteration   = 5\n",
    "        self.clip                      = 0.2\n",
    "        self.lr                        = 0.005\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"Generate time_steps_per_batch in multiple episodes each \n",
    "        of maximum length max_timesteps_per_episode\n",
    "        \"\"\"\n",
    "        # Batch data\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rgts = [] # Batch rewards-to-go\n",
    "        batch_lens = [] # Episiodic length in batch\n",
    "\n",
    "        current_timesteps = 0\n",
    "        while current_timesteps < self.timesteps_per_batch:\n",
    "            \n",
    "            # Initialization\n",
    "            ep_rews = [] # special format for rgts\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                # Collect observations\n",
    "                batch_obs.append(obs)\n",
    "                action, log_probs = self.get_action(obs)\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "\n",
    "                # Collect reward, action and log_prob\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_probs)\n",
    "\n",
    "                current_timesteps += 1\n",
    "                if done: break # If the agent completed the task finish\n",
    "\n",
    "            # Collect episodic length and rewards\n",
    "            current_timesteps += ep_t\n",
    "            batch_rews.append(ep_rews)            \n",
    "            batch_lens.append(ep_t + 1)\n",
    "\n",
    "        # Transform into desired format (Torch)\n",
    "        batch_obs       = torch.tensor(batch_obs, dtype = torch.float)\n",
    "        batch_acts      = torch.tensor(batch_acts, dtype = torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype = torch.float)\n",
    "\n",
    "        batch_rgts = self.compute_rgts(batch_rews)\n",
    "\n",
    "        return batch_obs,batch_acts, batch_log_probs, batch_rgts, batch_lens\n",
    "\n",
    "    def compute_rgts(self, batch_rews: list) -> torch.Tensor:\n",
    "        \"Compute rtg per episode per batch\"\n",
    "\n",
    "        batch_rgts = []\n",
    "\n",
    "        for ep_rews in batch_rews:\n",
    "            ep_rgts = []\n",
    "            discounted_sum = 0\n",
    "            for rew in reversed(batch_rews):\n",
    "                discounted_sum = rew + self.gamma * rew\n",
    "                ep_rgts.insert(0,discounted_sum) # We need the history of rgts at each time steps\n",
    "            \n",
    "            batch_rgts.extend(ep_rgts)\n",
    "        \n",
    "        return torch.tensor(batch_rgts, dtype = torch.float)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"Generate an action as a sample of a Multivariate normal distribution\"\n",
    "        # Query actor network for an action\n",
    "        mean = self.actor(obs) # Call NN (self.actor.forward(obs))\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "\n",
    "        # Generate sample from the distribution\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "\n",
    "    def learn(self, total_timesteps: int): \n",
    "\n",
    "        current_timestep = 0\n",
    "\n",
    "        while current_timestep < total_timesteps:\n",
    "\n",
    "            batch_obs,batch_acts, batch_log_probs, batch_rgts, batch_lens = self.rollout()\n",
    "            # Calculate V_(k,phi)\n",
    "            V_k, _ = self.evaluate(batch_obs)\n",
    "            # Calculate advantage\n",
    "            A_k = batch_rgts - V_k.detach() # Remove computational graph\n",
    "            # Normalize advantage function\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                # Calculate phi_theta(a_t|s_t)\n",
    "                _, cur_log_probs = self.evaluate(batch_obs)\n",
    "                # Ratio\n",
    "                ratios = torch.exp(cur_log_probs - batch_log_probs)\n",
    "                # Calculate surrogate loss\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "                actor_loss = (-torch.min(surr1,surr2)).mean()\n",
    "\n",
    "                # Calculate gradient and perform backward propagation\n",
    "                self.actor_opti.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_opti.step()\n",
    "\n",
    "        \n",
    "    def evaluate(self, batch_obs: torch.Tensor, batch_action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Value function at batch k: V_(k,phi) and log probs\n",
    "        We compute the prob of taking an action w.r.t current policy defined by the NN\n",
    "        \"\"\"\n",
    "        # We need the value at each state given the obs sequence of the batch\n",
    "        V = self.critic(batch_obs).squeeze()\n",
    "        # Compute log probs in the same fashion to get_action\n",
    "        mean = self.actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_action)\n",
    "\n",
    "        return V, log_probs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
