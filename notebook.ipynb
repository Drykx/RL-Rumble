{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a23794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert obs to tensor\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype = torch.float)\n",
    "        # Forward pass\n",
    "        obs = F.relu(self.fc1(obs))\n",
    "        obs = self.fc2(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca771873",
   "metadata": {},
   "source": [
    "**Architectural choices:**\n",
    "\n",
    "- Define the feedforwardNN class independently and inherit it from nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362095d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 59 (1581234488.py, line 62)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef get_action(self,obs)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 59\n"
     ]
    }
   ],
   "source": [
    "class PPO(nn.module):\n",
    "\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.act_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.init_parameters = self._init_hyperparameters()\n",
    "\n",
    "        self.actor = Feedforward(self.obs_dim,self.act_dim)\n",
    "        self.critic = Feedforward(self.obs_dim,1)\n",
    "\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = 4000\n",
    "        self.max_timesteps_per_episode = 1600 \n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"Generate time_steps_per_batch in multiple episodes each \n",
    "        of maximum length max_timesteps_per_episode\n",
    "        Retrieve MDP in torch format \n",
    "\n",
    "        of batch to retrieve the rewards \n",
    "        # wy; action, state_space ? \n",
    "        # Why different format between rewards and the rest\n",
    "        \"\"\"\n",
    "        # Batch data\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rgts = [] # Batch rewards-to-go\n",
    "        batch_lens = [] # Episiodic length in batch\n",
    "\n",
    "        current_timesteps = 0\n",
    "        while current_timesteps < self.timesteps_per_batch:\n",
    "            \n",
    "            # Initialization\n",
    "            ep_rews = [] # special format for rgts\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            ep_t = 0\n",
    "\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                # Collect observations\n",
    "                batch_obs.append(obs)\n",
    "                action, log_probs = self.get_action(obs)\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "\n",
    "                # Collect reward, action and log_prob\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_probs)\n",
    "\n",
    "                ep_t += 1\n",
    "                if done: break # If the agent completed the task finish\n",
    "\n",
    "            # Collect episodic length and rewards\n",
    "            current_timesteps += ep_t\n",
    "            batch_rews.append(ep_rews)            \n",
    "            batch_lens.append(ep_t + 1)\n",
    "\n",
    "        # Transform into desired format (Torch)\n",
    "        batch_obs =  torch.tensor(batch_obs, dtype = torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype = torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype = torch.float)\n",
    "\n",
    "        batch_rgts = self.compute_rgts(batch_rews)\n",
    "\n",
    "        return(batch_obs,batch_acts, batch_log_probs, batch_rgts, batch_lens)\n",
    "\n",
    "    def compute_rgts(self,batch_rews):\n",
    "\n",
    "\n",
    "    def __init(self,env, stdev = 0.5):\n",
    "        \"Create diagonal covariance matrix of stdev to sample actions from state\"\n",
    "\n",
    "        self.cov_vect = torch.full((size = self.act_dim), fill_value = stdev) # Stdev vector\n",
    "        self.cov_mat = torch.diag(self.cov_vect) # [act_dim, act_dim]\n",
    "\n",
    "    def get_action(self,obs):\n",
    "        \"Generate an action as a sample of a Multivariate normal distribution\"\n",
    "        # Query actor network for an action\n",
    "        mean = self.actor(obs) # Call NN (self.actor.forward(obs))\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "\n",
    "        # Generate sample from the distribution\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach().nump(), log_prob.detach()\n",
    "\n",
    "    def learn(self, total_timesteps): \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c7e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
